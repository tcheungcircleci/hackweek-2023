{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "SAGEMAKER_ROLE = 'arn:aws:iam::483285841698:role/service-role/AmazonSageMaker-ExecutionRole-20200612T174829'\n",
    "BUCKET = \"cci-data-science-devaccount\"\n",
    "PROFILE_NAME = \"dsa\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "boto_session = boto3.Session(\n",
    "    region_name=REGION,\n",
    "    profile_name=PROFILE_NAME,\n",
    ")\n",
    "\n",
    "sagemaker_client = boto_session.client(\n",
    "    service_name=\"sagemaker\"\n",
    ")\n",
    "\n",
    "sagemaker_runtime_client = boto_session.client(\n",
    "    service_name=\"sagemaker-runtime\",\n",
    ")\n",
    "\n",
    "s3_client = boto_session.client(\n",
    "    service_name=\"s3\",\n",
    ")\n",
    "\n",
    "sagemaker_session = sagemaker.Session(\n",
    "    boto_session = boto_session,\n",
    "    sagemaker_client = sagemaker_client,\n",
    "    sagemaker_runtime_client = sagemaker_runtime_client,\n",
    "    default_bucket = BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from itertools import chain\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "ds = load_dataset(\"bigcode/the-stack\", data_dir=\"data/yaml\", streaming=True, split=\"train\")\n",
    "\n",
    "train_list, test_list = [], []\n",
    "counter = 0\n",
    "\n",
    "for sample in iter(ds):\n",
    "    if '.circleci/config.yml' in sample['max_stars_repo_path']:\n",
    "        if counter <= 10000:\n",
    "            train_list.append(sample['content'])\n",
    "        elif counter <= 20000:\n",
    "            test_list.append(sample['content'])\n",
    "        else:\n",
    "            break\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cache_version_keys: &cache_version_keys\\n  CACHE_VERSION_OF_PROJECT_DEPS: v2\\n  CACHE_VERSION_OF_DANGER_CACHE: v1\\n\\ncache_keys:\\n  gradle_cache:\\n    primary: &primary_key_of_gradle_cache gradle-cache-{{ checksum \"~/CACHE_VERSION_OF_PROJECT_DEPS\" }}}-{{ checksum \"~/project_hash.txt\" }}\\n    keys: &all_keys_of_gradle_cache\\n      - *primary_key_of_gradle_cache\\n      - gradle-cache-{{ checksum \"~/CACHE_VERSION_OF_PROJECT_DEPS\" }}-\\n  danger_cache:\\n    primary: &primary_key_of_danger_cache danger-cache-{{ checksum \"~/CACHE_VERSION_OF_DANGER_CACHE\" }}-{{ checksum \"~/danger_cache\" }}\\n    keys: &all_keys_of_danger_cache\\n      - *primary_key_of_danger_cache\\n      - danger-cache-{{ checksum \"~/CACHE_VERSION_OF_DANGER_CACHE\" }}-\\n\\ndocker_env:\\n  android_defaults: &android_defaults\\n    working_directory: ~/conference-app-2019\\n    docker:\\n      - image: circleci/android:api-28-alpha\\n    environment:\\n      <<: *cache_version_keys\\n      JAVA_OPTS: \"-Xmx1024m\"\\n      GRADLE_OPTS: \\'-Dorg.gradle.jvmargs=\"-Xmx2048m -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError\" -Dorg.gradle.daemon=false -Dorg.gradle.parallel=false\\'\\n\\n  go_defaults: &go_defaults\\n    working_directory: ~/conference-app-2019\\n    docker:\\n      - image: jmatsu/dpg:v0.3.0\\n\\nshared_build_steps:\\n  - run: &init_bash\\n      name: Source the bash source\\n      command: |\\n        echo \"export REPOSITORY_ROOT=$(git rev-parse --show-toplevel)\" >> $BASH_ENV\\n        echo \"source $CIRCLE_WORKING_DIRECTORY/scripts/bash.source\" >> $BASH_ENV\\n        source $BASH_ENV\\n        init_ci_stuff\\n  - run: &download_dpg\\n      name: Download a binary file of dpg\\n      command: |\\n        cd $(eval echo $CIRCLE_WORKING_DIRECTORY)/scripts\\n        curl -sL \"https://raw.githubusercontent.com/jmatsu/dpg/master/install.bash\" | bash\\n        dpg -v\\n\\nversion: 2\\njobs:\\n  assemble_apk:\\n    <<: *android_defaults\\n    steps:\\n      - checkout\\n      - run: *init_bash\\n      - restore_cache: &restore_gradle_cache\\n          keys: *all_keys_of_gradle_cache\\n      - run: &download_all_dependencies\\n          name: Download Dependencies\\n          command: retry_command ./gradlew androidDependenciesExtra getDependencies\\n      - save_cache: &save_gradle_cache\\n          paths:\\n            - ~/.android\\n            - ~/.gradle\\n            - .gradle\\n          key: *primary_key_of_gradle_cache\\n      - run:\\n          name: Assemble apk from App Bundle\\n          command: |\\n            use_debug_keystore\\n            ./gradlew bundleStaging --offline\\n            create_universal_apk_from_aab.bash $(find frontend/android/build/outputs -name \"*.aab\" | head -1)\\n      - store_artifacts:\\n          path: frontend/android/build/outputs\\n      - store_artifacts:\\n          path: universal.apk # see UNIVERSAL_APK_PATH\\n      - run: *download_dpg\\n      - run:\\n          name: Upload an apk and create a distribution by app-manage procedure.\\n          command: |\\n            dpg procedure app-manage on-feature-branch --app \"$UNIVERSAL_APK_PATH\" --android | parse_dg_response\\n      - restore_cache: &restore_danger_cache\\n          keys: *all_keys_of_danger_cache\\n      - run: &bundle_install_for_danger\\n          name: Bundle install for Danger\\n          command: |\\n            cd scripts/danger\\n            bundle check || bundle install --clean\\n          when: always\\n      - save_cache: &save_danger_cache\\n          paths:\\n            - ./scripts/danger/vendor/bundle\\n          key: *primary_key_of_danger_cache\\n      - run:\\n          name: Compare apks with the latest production apk\\n          command: |\\n            if download_latest_apk \"$PWD/latest.apk\"; then\\n              export BUILT_APK_FILE_PATH=\"$UNIVERSAL_APK_PATH\"\\n              export OLD_BUILT_APK_FILE_PATH=\"$PWD/latest.apk\"\\n\\n              danger_compare_apks\\n            fi\\n  vital_check:\\n    <<: *android_defaults\\n    steps:\\n      - checkout\\n      - run: *init_bash\\n      - restore_cache: *restore_gradle_cache\\n      - run: *download_all_dependencies\\n      - save_cache: *save_gradle_cache\\n      - run: ./gradlew lintDebug testDebugUnitTest ktlint --continue --offline # we want to check lintRelease as well but RAM shortage... oh my...\\n      - run:\\n          name: Aggregate junit report files into one dir\\n          command: aggregate_junit_results\\n          when: always\\n      - store_test_results:\\n          path: test-results # see scripts/bash.source\\n          when: always\\n      - store_artifacts:\\n          path: reports # see scripts/bash.source\\n          when: always\\n      - restore_cache: *restore_danger_cache\\n      - run: *bundle_install_for_danger\\n      - run:\\n          name: Report lint results else\\n          command: danger_assertions\\n          when: always\\n      - save_cache: *save_danger_cache\\n\\n  destroy_distribution:\\n    <<: *go_defaults\\n    steps:\\n      - checkout\\n      - run: *init_bash\\n      - run:\\n          name: Destroy the associated distribution by app-manage procedure.\\n          command: |\\n            ./scripts/destroy_distribution\\n  placeholder:\\n    <<: *go_defaults\\n    steps:\\n      - run: echo Hello\\nworkflows_filter:\\n  only_production_ready: &only_production_ready\\n    filters:\\n      branches:\\n        only: /release/\\n  except_production_ready: &except_production_ready\\n    filters:\\n      branches:\\n        ignore: /release/\\n\\nworkflows:\\n  version: 2\\n  staging_or_feature:\\n    jobs:\\n      - assemble_apk: *except_production_ready\\n      - vital_check: *except_production_ready\\n      - destroy_distribution:\\n          filters:\\n            branches:\\n              only: /master/\\n  production_ready:\\n    jobs:\\n      - placeholder: *only_production_ready\\n',\n",
       " 'version: 2.1\\n\\norbs:\\n  rn: react-native-community/react-native@1.1.0\\n\\njobs:\\n  checkout_code:\\n    executor: rn/linux_js\\n    steps:\\n      - checkout\\n      - persist_to_workspace:\\n          root: .\\n          paths: .\\n  analyse:\\n    executor: rn/linux_js\\n    steps:\\n      - attach_workspace:\\n          at: .\\n      - rn/yarn_install\\n      - run:\\n          name: Lint JS Code (ESLint)\\n          command: yarn lint\\n      - run:\\n          name: Flow\\n          command: yarn flow-check\\n      - run:\\n          name: Unit Tests\\n          command: yarn test\\n\\nworkflows:\\n  test:\\n    jobs:\\n      - checkout_code\\n      - analyse:\\n          requires:\\n            - checkout_code\\n',\n",
       " 'version: 2.1\\njobs:\\n  python_steps:\\n    docker:\\n      - image: cimg/python:3.9.5\\n        environment:\\n          DATABASE_URL: sqlite:///trade_remedies_public/db.sqlite3\\n          DJANGO_SETTINGS_MODULE: config.settings.local\\n          DJANGO_SECRET_KEY: used_for_testing\\n          ALLOWED_HOSTS: \"*\"\\n    steps:\\n      - checkout\\n      - restore_cache:\\n          key: deps2-{{ .Branch }}-{{ checksum \"requirements/dev.txt\" }}\\n      - run:\\n          name: Install Python deps\\n          command: |\\n            python3 -m venv venv\\n            . venv/bin/activate\\n            pip install -r requirements/dev.txt\\n      - save_cache:\\n          key: deps2-{{ .Branch }}-{{ checksum \"requirements/dev.txt\" }}\\n          paths:\\n            - \"venv\"\\n      - run:\\n          name: Run tests\\n          command: |\\n            . venv/bin/activate\\n            cd trade_remedies_public\\n            python manage.py test\\n      - run:\\n          name: Run black\\n          command: |\\n            . venv/bin/activate\\n            cd trade_remedies_public\\n            black . --check\\n      - run:\\n          name: Run Flake8\\n          command: |\\n            . venv/bin/activate\\n            cd trade_remedies_public\\n            python -m flake8\\n  front_end:\\n    docker:\\n      - image: \\'circleci/node:lts\\'\\n    steps:\\n      - checkout\\n      - restore_cache:\\n          keys:\\n            - npm-dependencies-{{ checksum \"package-lock.json\" }}\\n      - run:\\n          name: Restore npm dependencies\\n          command: npm ci\\n      - run:\\n          name: Run prettier\\n          command: |\\n            npm run prettier\\n      - save_cache:\\n          paths:\\n            - node_modules\\n          key: npm-dependencies-{{ checksum \"package-lock.json\" }}\\n\\nworkflows:\\n  version: 2\\n  run_tests:\\n    jobs:\\n      - python_steps\\n      - front_end\\n',\n",
       " '# Javascript Node CircleCI 2.0 configuration file\\n#\\n# Check https://circleci.com/docs/2.0/language-javascript/ for more details\\n#\\nversion: 2\\njobs:\\n  build:\\n    docker:\\n      # specify the version you desire here\\n      - image: circleci/node:9.3.0\\n\\n      # Specify service dependencies here if necessary\\n      # CircleCI maintains a library of pre-built images\\n      # documented at https://circleci.com/docs/2.0/circleci-images/\\n      # - image: circleci/mongo:3.4.4\\n\\n    working_directory: ~/ng-compiler\\n\\n    steps:\\n      - checkout\\n\\n      # Download and cache dependencies\\n      - restore_cache:\\n          keys:\\n          - v1-dependencies-{{ checksum \"package.json\" }}\\n\\n      - run: npm install\\n\\n      - save_cache:\\n          paths:\\n            - node_modules\\n          key: v1-dependencies-{{ checksum \"package.json\" }}\\n\\n      # run tests!\\n      - run: npm test\\n      - store_artifacts:\\n          path: coverage\\n      - store_artifacts:\\n          path: junit\\n      - store_test_results:\\n          path: junit\\n\\n      # # upload coverage file to codecov\\n      # - run: ./node_modules/.bin/codecov\\n\\n      - deploy:\\n          name: npm run build\\n          command: |\\n            if [ \"${CIRCLE_BRANCH}\" == \"master\" ]; then\\n              npm run build\\n            fi\\n      - deploy:\\n          name: npm run semantic-release\\n          command: |\\n            if [ \"${CIRCLE_BRANCH}\" == \"master\" ]; then\\n              npm run semantic-release\\n            fi\\n',\n",
       " 'version: 2.1\\n\\ncommands:\\n    tests:\\n        description: \"Execute tests\"\\n        steps:\\n            - run:\\n                  name: \"Execute: tests/run.sh\"\\n                  command: sh ./tests/run.sh\\n\\nexecutors:\\n    php7_1:\\n        docker:\\n            - image: php:7.1-cli-alpine\\n        working_directory: ~/repository\\n\\n    php7_2:\\n        docker:\\n            - image: php:7.2-cli-alpine\\n        working_directory: ~/repository\\n\\n    php7_3:\\n        docker:\\n            - image: php:7.3-cli-alpine\\n        working_directory: ~/repository\\n\\n    php7_4:\\n        docker:\\n            - image: php:7.4-cli-alpine\\n        working_directory: ~/repository\\n\\n    php8_0:\\n        docker:\\n            - image: php:8.0-cli-alpine\\n        working_directory: ~/repository\\n\\njobs:\\n    tests_7_1:\\n        executor: php7_1\\n        steps:\\n            - checkout\\n            - tests\\n\\n    tests_7_2:\\n        executor: php7_2\\n        steps:\\n            - checkout\\n            - tests\\n\\n    tests_7_3:\\n        executor: php7_3\\n        steps:\\n            - checkout\\n            - tests\\n\\n    tests_7_4:\\n        executor: php7_4\\n        steps:\\n            - checkout\\n            - tests\\n\\n    tests_8_0:\\n        executor: php8_0\\n        steps:\\n            - checkout\\n            - tests\\n\\nworkflows:\\n    version: 2.1\\n    Tests:\\n        jobs:\\n            - tests_7_1\\n            - tests_7_2\\n            - tests_7_3\\n            - tests_7_4\\n            - tests_8_0\\n']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shared_configs:\\n  simple_job_steps: &simple_job_steps\\n    - checkout\\n    - run:\\n        name: Run tests\\n        command: |\\n          make deps test\\n\\n\\n# Use the latest 2.1 version of CircleCI pipeline process engine. See: https://circleci.com/docs/2.0/configuration-reference\\nversion: 2.1\\njobs:\\n  build-1-12:\\n    working_directory: ~/repo\\n    docker:\\n      - image: circleci/golang:1.12\\n    environment:\\n      GO111MODULE: \"on\"\\n    steps: *simple_job_steps\\n\\n  build-1-13:\\n    working_directory: ~/repo\\n    docker:\\n      - image: circleci/golang:1.13\\n    steps: *simple_job_steps\\n\\n  build-1-14:\\n    working_directory: ~/repo\\n    docker:\\n      - image: circleci/golang:1.14\\n    steps: *simple_job_steps\\n\\n  build-1-15:\\n    working_directory: ~/repo\\n    docker:\\n      - image: circleci/golang:1.15\\n    environment:\\n      GO111MODULE: \"on\"\\n    steps:\\n      - checkout\\n      - restore_cache:\\n          keys:\\n            - go-mod-v4-{{ checksum \"go.sum\" }}\\n      - run:\\n          name: Install Dependencies\\n          command: go mod download\\n      - save_cache:\\n          key: go-mod-v4-{{ checksum \"go.sum\" }}\\n          paths:\\n            - \"/go/pkg/mod\"\\n      - run:\\n          name: Run tests\\n          command: |\\n            #mkdir -p /tmp/test-reports\\n            #gotestsum --junitfile /tmp/test-reports/unit-tests.xml\\n            make ci\\n    #- store_test_results:\\n    #    path: /tmp/test-reports\\n\\n  build-1-16:\\n    working_directory: ~/repo\\n    docker:\\n      - image: circleci/golang:1.16\\n    steps: *simple_job_steps\\n\\nworkflows:\\n  pr-build-test:\\n    jobs:\\n      - build-1-12\\n      - build-1-13\\n      - build-1-14\\n      - build-1-15\\n      - build-1-16\\n',\n",
       " '\\nversion: 2.1\\n\\ncommands:\\n  greeting:\\n      parameters:\\n         to:\\n           default: \"world\"\\n           type: string\\n      steps:\\n         - run: echo \"Hello <<parameters.to>>\"\\n\\n  test:\\n    steps:\\n      - run: \\n          name: \"gotestsum install\"\\n          command: go get gotest.tools/gotestsum\\n      - run: \\n          name: \"create test results dir\"\\n          command: mkdir -p test-results\\n      - run: \\n          name: \"execute tests\"\\n          command: gotestsum --format dots --junitfile test-results/unit_tests.xml ./...\\n      - store_test_results:\\n          path: test-results\\n\\njobs:\\n  sanity:\\n    docker:\\n      # specify the version\\n      - image: circleci/golang:1.14\\n    working_directory: /go/src/github.com/curious-kitten/forge\\n    steps:\\n      - checkout\\n      - test\\n      \\n\\nworkflows:\\n  version: 2\\n  test-integration:\\n    jobs:\\n      - sanity\\n',\n",
       " 'version: 2.1\\n\\ncommands:\\n  docker_hub_login:\\n    description: Login to Docker Hub\\n    steps:\\n      - run:\\n          name: Docker Hub login\\n          command: docker login -u \"$DOCKER_USERNAME\" -p \"$DOCKER_PASSWORD\"\\n  configure_git:\\n    description: Configure Git\\n    steps:\\n      - run:\\n          name: Delete gitconfig\\n          command: rm -rf ~/.gitconfig\\n      - run:\\n          name: Configure Git user\\n          command: |\\n            git config --global user.email \"ci@healthlabs.com\"\\n            git config --global user.name \"Circle CI\"\\n      - run:\\n          name: Configure Git to use SSH instead of HTTP\\n          command: |\\n            git config --global url.git@github.com:.insteadOf git://github.com/\\n      - run:\\n          name: Add in submodules\\n          command: git submodule update --init\\n\\n  pull-merge-restore:\\n    description: \"Bring the repository up-to-date with master and restore the cache\"\\n    steps:\\n      - checkout\\n      - restore_cache:\\n          key: composer-{{ checksum \"composer.lock\" }}\\n      - configure_git\\n      - run:\\n          name: Merge master\\n          command: git fetch origin && git merge origin/master --no-edit\\n      - restore_cache:\\n          key: composer-{{ checksum \"composer.lock\" }}\\n\\n  build-and-test:\\n    description: \"Build the project.\"\\n    steps:\\n      - run:\\n          name: Initialize the project\\n          command: |\\n            mkdir -p /tmp/artifacts\\n            bin/init_project\\n      - save_cache:\\n          key: composer-{{ checksum \"composer.lock\" }}\\n          paths:\\n            - ~/.composer/cache\\n      - run:\\n          name: Unit Tests\\n          command: phpunit --log-junit artifacts/phpunit.junit.xml\\n      - store_test_results:\\n          path: /tmp/artifacts\\n      - store_artifacts:\\n          path: /tmp/artifacts\\n\\nexecutors:\\n  default:\\n    machine:\\n      image: ubuntu-2004:202010-01\\n    environment:\\n      PATH: /opt/circleci/.pyenv/shims:./bin:./vendor/bin:./submodules/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:./node_modules/.bin\\n\\njobs:\\n  test-php7_3:\\n    working_directory: /tmp/laravel_sodium\\n    parallelism: 1\\n    executor: default\\n    environment:\\n      PHP_VERSION: 7.3\\n    steps:\\n      - pull-merge-restore\\n      - docker_hub_login\\n      - build-and-test\\n  test-php7_4:\\n    working_directory: /tmp/laravel_sodium\\n    parallelism: 1\\n    executor: default\\n    environment:\\n      PHP_VERSION: 7.4\\n    steps:\\n      - pull-merge-restore\\n      - docker_hub_login\\n      - build-and-test\\n\\nworkflows:\\n  version: 2\\n  build-and-test:\\n    jobs:\\n      - test-php7_3\\n      - test-php7_4\\n',\n",
       " 'orbs: # declare what orbs we are going to use\\n  node: circleci/node@2.0.2 # the node orb provides common node-related configuration \\n\\nversion: 2.1 # using 2.1 provides access to orbs and other features\\n\\nworkflows:\\n  matrix-tests:\\n    jobs:\\n      - node/test:\\n          version: 13.11.0\\n      - node/test:\\n          version: 12.16.0\\n      - node/test:\\n          version: 10.19.0\\n\\n',\n",
       " 'version: 2.1\\norbs:\\n  node: circleci/node@4.1.0\\n  aws-ecr: circleci/aws-ecr@6.9.1\\n  aws-eks: circleci/aws-eks@1.0.3\\n  aws-cli: circleci/aws-cli@1.4.0\\n  kubernetes: circleci/kubernetes@0.11.2\\n\\njobs:\\n  run_functional_tests:\\n    executor:\\n      name: node/default\\n    steps:\\n      - checkout\\n      - node/install-packages\\n      - run:\\n          name: Gauge tests\\n          command: |\\n            sudo apt-get update\\n            sudo apt-get install -y git-all gconf-service libasound2 libatk1.0-0 libc6 libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 libglib2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxss1 libxtst6 ca-certificates fonts-liberation libgbm-dev libappindicator1 libnss3 lsb-release xdg-utils wget\\n            sudo rm -rf /var/lib/apt/lists/*\\n            npm run functional_test -- --env test\\n\\n  approve_to_deploy:\\n    docker:\\n      - image: circleci/node:13.4\\n    steps:\\n      - run: echo \"Hold for approval\"\\n\\n  db_migrate_aws:\\n    parameters:\\n      db_name:\\n        type: string\\n    executor:\\n      name: node/default\\n    steps:\\n      - checkout\\n      - node/install-packages\\n      - run:\\n          name: Run DB Migrate\\n          command: |\\n            echo << parameters.db_name >>\\n            export DATABASE_URL=postgresql://${DEV_DB_USER}:${DB_PASS_AWS}@${DB_HOST_AWS}/<< parameters.db_name >>\\n            echo << parameters.db_name >>\\n            echo $DATABASE_URL\\n            npx db-migrate up\\n\\n  eks_fargate_deployment:\\n    executor: aws-eks/python3\\n    parameters:\\n      cluster-name:\\n        description: |\\n          Name of the EKS cluster\\n        type: string\\n      env-name:\\n        description: |\\n          Name of the env\\n        type: string\\n\\n    steps:\\n      - checkout\\n      - aws-cli/setup:\\n          profile-name: circle-ci\\n      - kubernetes/install\\n      - run:\\n          command: |\\n            aws eks --region ap-south-1 update-kubeconfig --name << parameters.cluster-name >>\\n            ENV_IMAGE_NAME=${AWS_ECR_ACCOUNT_URL}/${REPO}:<< parameters.env-name >>\\n            kubectl -n << parameters.env-name >> apply  -f ${HOME}/project/deployments/k8s/autoscaler_<< parameters.env-name >>.yaml\\n\\n            sed -e \"s|IMAGE_NAME|$ENV_IMAGE_NAME|g;s/CIRCLE_SHA1/$CIRCLE_SHA1/g\" ${HOME}/project/deployments/k8s/deployment.yaml | kubectl -n << parameters.env-name >> apply  -f -\\n\\n          name: Deploy container\\n\\nworkflows:\\n  build_test_deploy:\\n    jobs:\\n      - node/test:\\n          version: \"13.4\"\\n\\n      - db_migrate_aws:\\n          name: migrate_dev_aws\\n          db_name: ${DEV_DB_NAME}\\n          requires:\\n             - node/test\\n          filters:\\n            branches:\\n              only: master\\n\\n      - aws-ecr/build-and-push-image:\\n          name: push_dev_image\\n          repo: ${REPO}\\n          tag: dev\\n          requires:\\n            #  - node/test\\n            - migrate_dev_aws\\n\\n      - eks_fargate_deployment:\\n          name: dev_deployment_eks\\n          cluster-name: ${CLUSTER_NAME}\\n          env-name: dev\\n          requires:\\n            - push_dev_image\\n\\n      - approve_to_deploy:\\n          name: approve_test_deployment_eks\\n          type: approval\\n          requires:\\n            - dev_deployment_eks\\n\\n      - db_migrate_aws:\\n          name: migrate_test_aws\\n          db_name: ${TEST_DB_NAME}\\n          requires:\\n            - approve_test_deployment_eks\\n\\n      - aws-ecr/build-and-push-image:\\n          name: push_test_image\\n          repo: ${REPO}\\n          tag: test\\n          requires:\\n            - migrate_test_aws\\n\\n      - eks_fargate_deployment:\\n          name: test_deployment_eks\\n          cluster-name: ${CLUSTER_NAME}\\n          env-name: test\\n          requires:\\n            - push_test_image\\n\\n      - run_functional_tests:\\n          name: functional_tests\\n          requires:\\n            - test_deployment_eks\\n\\n      - approve_to_deploy:\\n          name: approve_prod_deployment_eks\\n          type: approval\\n          requires:\\n            - functional_tests\\n\\n      - db_migrate_aws:\\n          name: migrate_prod_aws\\n          db_name: ${DB_NAME}\\n          requires:\\n            - approve_prod_deployment_eks\\n\\n      - aws-ecr/build-and-push-image:\\n          name: push_prod_image\\n          repo: ${REPO}\\n          tag: prod\\n          requires:\\n            - migrate_prod_aws\\n\\n      - eks_fargate_deployment:\\n          name: prod_deployment_eks\\n          cluster-name: ${CLUSTER_NAME}\\n          env-name: prod\\n          requires:\\n            - push_prod_image\\n']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train['content'] = train_list\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['content'] = test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'N2211F02SA7XGERK',\n",
       "  'HostId': 'zHVavKRhzctY1HgohdE8QjTN2kJHGE/0aD2bxLHG7yTMbTxjGZNqtk0Xm2UGBf0upoc8hhaP86g=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'zHVavKRhzctY1HgohdE8QjTN2kJHGE/0aD2bxLHG7yTMbTxjGZNqtk0Xm2UGBf0upoc8hhaP86g=',\n",
       "   'x-amz-request-id': 'N2211F02SA7XGERK',\n",
       "   'date': 'Tue, 13 Jun 2023 20:46:34 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"3b0d13b22a235f38a1953bdd644ab50f\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"3b0d13b22a235f38a1953bdd644ab50f\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_buffer = StringIO()\n",
    "df_train.to_csv(csv_buffer, index=False)\n",
    "s3_client.put_object(\n",
    "    Bucket=BUCKET,\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    Key=f'hackweek-2023/train/train.csv'\n",
    ")\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "df_test.to_csv(csv_buffer, index=False)\n",
    "s3_client.put_object(\n",
    "    Bucket=BUCKET,\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    Key=f'hackweek-2023/test/test.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = s3_client.get_object(\n",
    "    Bucket=BUCKET,\n",
    "    Key=f'hackweek-2023/train/train.csv'\n",
    ")\n",
    "df_train_from_s3 = pd.read_csv(BytesIO(obj['Body'].read()))\n",
    "\n",
    "obj = s3_client.get_object(\n",
    "    Bucket=BUCKET,\n",
    "    Key=f'hackweek-2023/test/test.csv'\n",
    ")\n",
    "df_test_from_s3 = pd.read_csv(BytesIO(obj['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cache_version_keys: &amp;cache_version_keys\\n  CAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>version: 2.1\\n\\norbs:\\n  rn: react-native-comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>version: 2.1\\njobs:\\n  python_steps:\\n    dock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Javascript Node CircleCI 2.0 configuration f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>version: 2.1\\n\\ncommands:\\n    tests:\\n       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td># CircleCI Pre-Built Docker Images\\n# - https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>version: 2\\n\\njobs:\\n  deploy:\\n    docker:\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>jobs:\\n  test:\\n    docker:\\n      - image: no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>version: 2.1\\njobs:\\n  build:\\n    docker:\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>version: 2\\njobs:\\n  build:\\n    working_direc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10001 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "0      cache_version_keys: &cache_version_keys\\n  CAC...\n",
       "1      version: 2.1\\n\\norbs:\\n  rn: react-native-comm...\n",
       "2      version: 2.1\\njobs:\\n  python_steps:\\n    dock...\n",
       "3      # Javascript Node CircleCI 2.0 configuration f...\n",
       "4      version: 2.1\\n\\ncommands:\\n    tests:\\n       ...\n",
       "...                                                  ...\n",
       "9996   # CircleCI Pre-Built Docker Images\\n# - https:...\n",
       "9997   version: 2\\n\\njobs:\\n  deploy:\\n    docker:\\n ...\n",
       "9998   jobs:\\n  test:\\n    docker:\\n      - image: no...\n",
       "9999   version: 2.1\\njobs:\\n  build:\\n    docker:\\n  ...\n",
       "10000  version: 2\\njobs:\\n  build:\\n    working_direc...\n",
       "\n",
       "[10001 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_from_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shared_configs:\\n  simple_job_steps: &amp;simple_j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nversion: 2.1\\n\\ncommands:\\n  greeting:\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>version: 2.1\\n\\ncommands:\\n  docker_hub_login:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orbs: # declare what orbs we are going to use\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>version: 2.1\\norbs:\\n  node: circleci/node@4.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>version: \"2.1\"\\ncontext: pypi\\nworkflows:\\n  b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>version: 2.1\\n# This CircleCI orb doesn't seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>version: 2\\nbuild:\\n  machine:\\n    java: orac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>---\\ndefaults:\\n  defaults: &amp;defaults\\n    wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td># Javascript Node CircleCI 2.0 configuration f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content\n",
       "0     shared_configs:\\n  simple_job_steps: &simple_j...\n",
       "1     \\nversion: 2.1\\n\\ncommands:\\n  greeting:\\n    ...\n",
       "2     version: 2.1\\n\\ncommands:\\n  docker_hub_login:...\n",
       "3     orbs: # declare what orbs we are going to use\\...\n",
       "4     version: 2.1\\norbs:\\n  node: circleci/node@4.1...\n",
       "...                                                 ...\n",
       "9995  version: \"2.1\"\\ncontext: pypi\\nworkflows:\\n  b...\n",
       "9996  version: 2.1\\n# This CircleCI orb doesn't seem...\n",
       "9997  version: 2\\nbuild:\\n  machine:\\n    java: orac...\n",
       "9998  ---\\ndefaults:\\n  defaults: &defaults\\n    wor...\n",
       "9999  # Javascript Node CircleCI 2.0 configuration f...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_from_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content'],\n",
       "    num_rows: 10001\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict({\"content\": df_train_from_s3.values.tolist()})\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "SAGEMAKER_ROLE = 'arn:aws:iam::483285841698:role/service-role/AmazonSageMaker-ExecutionRole-20200612T174829'\n",
    "BUCKET = \"cci-data-science-devaccount\"\n",
    "PROFILE_NAME = \"dsa\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "boto_session = boto3.Session(\n",
    "    region_name=REGION,\n",
    "    profile_name=PROFILE_NAME,\n",
    ")\n",
    "\n",
    "sagemaker_client = boto_session.client(\n",
    "    service_name=\"sagemaker\"\n",
    ")\n",
    "\n",
    "sagemaker_runtime_client = boto_session.client(\n",
    "    service_name=\"sagemaker-runtime\",\n",
    ")\n",
    "\n",
    "s3_client = boto_session.client(\n",
    "    service_name=\"s3\",\n",
    ")\n",
    "\n",
    "sagemaker_session = sagemaker.Session(\n",
    "    boto_session = boto_session,\n",
    "    sagemaker_client = sagemaker_client,\n",
    "    sagemaker_runtime_client = sagemaker_runtime_client,\n",
    "    default_bucket = BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters = {\n",
    "\t'model_name':'huggingface/CodeBERTa-small-v1',\n",
    "\t'max_samples':10\n",
    "\t# add your remaining hyperparameters\n",
    "\t# more info here https://github.com/huggingface/transformers/tree/v4.26.0/examples/pytorch/language-modeling\n",
    "}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "\tentry_point='train.py',\n",
    "\tsource_dir='./scripts',\n",
    "\t# instance_type='ml.p3.2xlarge',\n",
    " \tinstance_type='ml.g4dn.2xlarge',\n",
    "\tinstance_count=1,\n",
    "\trole=SAGEMAKER_ROLE,\n",
    "\tsagemaker_session=sagemaker_session,\n",
    "\ttransformers_version='4.26',\n",
    "\tpytorch_version='1.13',\n",
    "\tpy_version='py39',\n",
    "\thyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "training_input_path = 's3://cci-data-science-devaccount/hackweek-2023/train'\n",
    "test_input_path = 's3://cci-data-science-devaccount/hackweek-2023/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-06-14-19-16-30-209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-14 19:16:31 Starting - Starting the training job...\n",
      "2023-06-14 19:16:47 Starting - Preparing the instances for training......\n",
      "2023-06-14 19:18:01 Downloading - Downloading input data......\n",
      "2023-06-14 19:18:57 Training - Downloading the training image.........\n",
      "2023-06-14 19:20:32 Training - Training image download completed. Training in progress....bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-06-14 19:20:58,700 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-06-14 19:20:58,720 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-06-14 19:20:58,731 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-06-14 19:20:58,733 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-06-14 19:20:58,981 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.9 -m pip install -r requirements.txt\n",
      "Collecting evaluate\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 1)) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (3.8.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 1)) (2022.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate->-r requirements.txt (line 1)) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-06-14 19:21:01,709 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-06-14 19:21:01,710 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-06-14 19:21:01,732 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-06-14 19:21:01,764 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-06-14 19:21:01,797 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-06-14 19:21:01,809 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_samples\": 10,\n",
      "        \"model_name\": \"huggingface/CodeBERTa-small-v1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-06-14-19-16-30-209\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://cci-data-science-devaccount/huggingface-pytorch-training-2023-06-14-19-16-30-209/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"max_samples\":10,\"model_name\":\"huggingface/CodeBERTa-small-v1\"}\n",
      "SM_USER_ENTRY_POINT=train.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://cci-data-science-devaccount/huggingface-pytorch-training-2023-06-14-19-16-30-209/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"max_samples\":10,\"model_name\":\"huggingface/CodeBERTa-small-v1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-06-14-19-16-30-209\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://cci-data-science-devaccount/huggingface-pytorch-training-2023-06-14-19-16-30-209/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "SM_USER_ARGS=[\"--max_samples\",\"10\",\"--model_name\",\"huggingface/CodeBERTa-small-v1\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_MAX_SAMPLES=10\n",
      "SM_HP_MODEL_NAME=huggingface/CodeBERTa-small-v1\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.9 train.py --max_samples 10 --model_name huggingface/CodeBERTa-small-v1\n",
      "[2023-06-14 19:21:03.647: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "2023-06-14 19:21:03,653 root         INFO     Using NamedTuple = typing._NamedTuple instead.\n",
      "2023-06-14 19:21:03,678 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 480/480 [00:00<00:00, 67.9kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/336M [00:00<?, ?B/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:   6%|▌         | 21.0M/336M [00:00<00:01, 206MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  16%|█▌        | 52.4M/336M [00:00<00:01, 261MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  25%|██▍       | 83.9M/336M [00:00<00:00, 283MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  34%|███▍      | 115M/336M [00:00<00:00, 291MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  44%|████▎     | 147M/336M [00:00<00:00, 296MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  53%|█████▎    | 178M/336M [00:00<00:00, 298MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  62%|██████▏   | 210M/336M [00:00<00:00, 298MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  72%|███████▏  | 241M/336M [00:00<00:00, 302MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  81%|████████  | 273M/336M [00:00<00:00, 302MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";:  90%|█████████ | 304M/336M [00:01<00:00, 293MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|█████████▉| 336M/336M [00:01<00:00, 274MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 336M/336M [00:01<00:00, 284MB/s]\n",
      "Downloading (…)okenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 19.0/19.0 [00:00<00:00, 7.98kB/s]\n",
      "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/994k [00:00<?, ?B/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 994k/994k [00:00<00:00, 87.2MB/s]\n",
      "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/483k [00:00<?, ?B/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 483k/483k [00:00<00:00, 67.4MB/s]\n",
      "Running tokenizer on every text in train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on every text in train dataset: 100%|██████████| 1/1 [00:00<00:00, 70.58ba/s]\n",
      "Running tokenizer on every text in test dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on every text in test dataset: 100%|██████████| 1/1 [00:00<00:00, 125.11ba/s]\n",
      "Grouping texts in chunks of 512:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Grouping texts in chunks of 512: 100%|██████████| 1/1 [00:00<00:00, 92.87ba/s]\n",
      "Grouping texts in chunks of 512:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Grouping texts in chunks of 512: 100%|██████████| 1/1 [00:00<00:00, 126.67ba/s]\n",
      "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 4.74MB/s]\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "Num examples = 12\n",
      "***** Running training *****\n",
      "  Num examples = 12\n",
      "Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "Number of trainable parameters = 83504416\n",
      "Number of trainable parameters = 83504416\n",
      "0%|          | 0/3 [00:00<?, ?it/s]\n",
      "[2023-06-14 19:21:13.767: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "2023-06-14 19:21:13,772 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-06-14 19:21:13.800 algo-1:55 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-06-14 19:21:13.832 algo-1:55 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-06-14 19:21:13.832 algo-1:55 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-06-14 19:21:13.833 algo-1:55 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-06-14 19:21:13.833 algo-1:55 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-06-14 19:21:13.833 algo-1:55 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "33%|███▎      | 1/3 [00:02<00:05,  2.56s/it]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8\n",
      "Num examples = 8\n",
      "  Batch size = 32\n",
      "Batch size = 32\n",
      "0%|          | 0/1 [00:00<?, ?it/s]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 1.4641590118408203, 'eval_accuracy': 0.7371048252911814, 'eval_runtime': 0.2215, 'eval_samples_per_second': 36.118, 'eval_steps_per_second': 4.515, 'epoch': 1.0}\n",
      "100%|██████████| 1/1 [00:00<00:00, 96.32it/s]#033[A#015 33%|███▎      | 1/3 [00:02<00:05,  2.56s/it]\n",
      "#033[A\n",
      "67%|██████▋   | 2/3 [00:03<00:01,  1.73s/it]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "***** Running Evaluation *****\n",
      "Num examples = 8\n",
      "  Batch size = 32\n",
      "Num examples = 8\n",
      "  Batch size = 32\n",
      "0%|          | 0/1 [00:00<?, ?it/s]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 1.7245317697525024, 'eval_accuracy': 0.7332214765100671, 'eval_runtime': 0.2259, 'eval_samples_per_second': 35.413, 'eval_steps_per_second': 4.427, 'epoch': 2.0}\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.31it/s]#033[A#015 67%|██████▋   | 2/3 [00:03<00:01,  1.73s/it]\n",
      "#033[A\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.47s/it]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "***** Running Evaluation *****\n",
      "Num examples = 8\n",
      "  Batch size = 32\n",
      "Num examples = 8\n",
      "  Batch size = 32\n",
      "0%|          | 0/1 [00:00<?, ?it/s]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 1.7095760107040405, 'eval_accuracy': 0.7288401253918495, 'eval_runtime': 0.2257, 'eval_samples_per_second': 35.445, 'eval_steps_per_second': 4.431, 'epoch': 3.0}\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.47s/it]\n",
      "#015100%|██████████| 1/1 [00:00<00:00, 99.60it/s]#033[A\n",
      "#033[A\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "{'train_runtime': 5.0881, 'train_samples_per_second': 7.075, 'train_steps_per_second': 0.59, 'train_loss': 1.6526637077331543, 'epoch': 3.0}\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.47s/it]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.70s/it]\n",
      "Saving model checkpoint to /opt/ml/model\n",
      "Saving model checkpoint to /opt/ml/model\n",
      "Configuration saved in /opt/ml/model/config.json\n",
      "Configuration saved in /opt/ml/model/config.json\n",
      "Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "train_loss               =     1.6527\n",
      "  train_runtime            = 0:00:05.08\n",
      "  train_samples            =         12\n",
      "  train_samples_per_second =      7.075\n",
      "  train_steps_per_second   =       0.59\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "***** Running Evaluation *****\n",
      "Num examples = 8\n",
      "  Batch size = 32\n",
      "Num examples = 8\n",
      "  Batch size = 32\n",
      "0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.76it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =     0.7368\n",
      "  eval_loss               =     1.6255\n",
      "  eval_runtime            = 0:00:00.20\n",
      "  eval_samples            =          8\n",
      "  eval_samples_per_second =     38.455\n",
      "  eval_steps_per_second   =      4.807\n",
      "  perplexity              =     5.0812\n",
      "***** Eval results *****\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7368421052631579}]}\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7368421052631579}]}\n",
      "2023-06-14 19:21:20,314 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-06-14 19:21:20,314 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-06-14 19:21:20,315 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2023-06-14 19:21:28 Uploading - Uploading generated training model\n",
      "2023-06-14 19:22:14 Completed - Training job completed\n",
      "Training seconds: 252\n",
      "Billable seconds: 252\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-inference-2023-06-14-19-23-24-560\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-inference-2023-06-14-19-23-25-375\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-inference-2023-06-14-19-23-25-375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "env = {'HF_TASK': 'fill-mask'}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://cci-data-science-devaccount/huggingface-pytorch-training-2023-06-14-19-16-30-209/output/model.tar.gz\",\n",
    "   role=SAGEMAKER_ROLE,\n",
    "   env=env,\n",
    "   sagemaker_session=sagemaker_session,\n",
    "   transformers_version=\"4.26\",\n",
    "   pytorch_version=\"1.13\",\n",
    "   py_version='py39',\n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"/opt/ml/model does not appear to have a file named config.json. Checkout \\u0027https://huggingface.co//opt/ml/model/None\\u0027 for available files.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-06-14-18-30-26-554 in account 483285841698 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m fillmask_input \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mversion: 2.1\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m      - test  \u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39m}\n\u001b[0;32m---> 27\u001b[0m predictor\u001b[39m.\u001b[39;49mpredict(fillmask_input)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/hackweek-2023-rkSGHFfB-py3.11/lib/python3.11/site-packages/sagemaker/base_predictor.py:167\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m        as is.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_request_args(\n\u001b[1;32m    165\u001b[0m     data, initial_args, target_model, target_variant, inference_id\n\u001b[1;32m    166\u001b[0m )\n\u001b[0;32m--> 167\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49msagemaker_runtime_client\u001b[39m.\u001b[39;49minvoke_endpoint(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_args)\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/hackweek-2023-rkSGHFfB-py3.11/lib/python3.11/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/hackweek-2023-rkSGHFfB-py3.11/lib/python3.11/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"/opt/ml/model does not appear to have a file named config.json. Checkout \\u0027https://huggingface.co//opt/ml/model/None\\u0027 for available files.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-06-14-18-30-26-554 in account 483285841698 for more information."
     ]
    }
   ],
   "source": [
    "fillmask_input = {\"inputs\": \"\"\"\n",
    "version: 2.1\n",
    "\n",
    "# Define the jobs we want to run for this project\n",
    "<mask>:\n",
    "  build:\n",
    "    docker:\n",
    "      - image: cimg/base:2023.03\n",
    "    steps:\n",
    "      - checkout\n",
    "      - run: echo \"this is the build job\"\n",
    "  test:\n",
    "    docker:\n",
    "      - image: cimg/base:2023.03\n",
    "    steps:\n",
    "      - checkout\n",
    "      - run: echo \"this is the test job\"\n",
    "\n",
    "# Orchestrate our job run sequence\n",
    "workflows:\n",
    "  build_and_test:\n",
    "    jobs:\n",
    "      - build\n",
    "      - test  \n",
    "\"\"\"}\n",
    "\n",
    "predictor.predict(fillmask_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackweek-2023-rkSGHFfB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
